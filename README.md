
# ğŸ§© Multi-Agent Scaffold (Planner â†’ Executor â†’ Critic)

A tiny, test-covered scaffold that turns **natural language instructions** into a **plan**, chooses the right **tool**, executes it, and **checks the result**.  
Itâ€™s like a baby AI agent: **plan â†’ act â†’ review**.

---

## âœ¨ Features
- **Planner** creates a 3-step plan:
  1. Clarify the goal  
  2. Choose a tool (`math` or `echo`)  
  3. Execute & summarize  
- **Executor** runs the chosen tool, with a **max-attempts guard** to avoid infinite retries  
- **Critic** verifies acceptance criteria (e.g. â€œsummary must mention the resultâ€)  
- **Orchestrator** ties everything together with a dynamic tool registry  
- **Tools**:  
  - `math` â†’ supports add, subtract, multiply, divide (with divide-by-zero protection)  
  - `echo` â†’ repeats your text back  

---

## ğŸ§± Project structure

.
â”œâ”€â”€ agents/               # planner, executor, critic, router
â”œâ”€â”€ core/                 # orchestrator, shared state
â”œâ”€â”€ tools/                # echo + math tool implementations
â”œâ”€â”€ tests/                # pytest suite
â”œâ”€â”€ app.py                # CLI entrypoint
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

---

## ğŸš€ Quickstart

### 1. Clone & install
```bash
git clone https://github.com/YOUR_USER/multiagent.git
cd multiagent
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

2. Run a prompt

python app.py "Please add 5 and 7"

Expected output (simplified):

Plan: [
  {"id": "plan-2", "tool": "math", "inputs": {"op": "add", "a": 5.0, "b": 7.0}},
  {"id": "plan-3", "result": {"tool_output": 12.0, "summary": "Computed 5.0 + 7.0 = 12.0. Summary: The result is 12.0."}}
]
Outcome: {"complete": true, "score": 1.0}
Trace summary: steps=3 done=3 status=complete


â¸»

ğŸ§ª Run tests

pytest -q

All tests should pass:
	â€¢	âœ… Happy path: 5 + 7 = 12
	â€¢	âœ… Echo fallback
	â€¢	âœ… Division by zero handled as error
	â€¢	âœ… Max-attempts guard prevents infinite retries

â¸»

ğŸ”§ Configuration
	â€¢	Optional OpenAI reasoning in agents/executor.py if you set OPENAI_API_KEY in the environment.
	â€¢	Defaults to offline mode with deterministic responses.
	â€¢	Config values (like tool registry) can be moved to config/default.yaml.

â¸»

ğŸ› ï¸ Extending
	1.	Add new tools in tools/ (e.g., search, files, code).
	2.	Register them in core/orchestrator.py under _TOOL_REGISTRY.
	3.	Teach the planner/executor to detect when to use them.
	4.	Add new pytest cases in tests/.

â¸»

ğŸ“œ License

MIT

â¸»

ğŸ‘©â€ğŸ’» Contributing

See CONTRIBUTING.md.
Please open issues or PRs for ideas, improvements, or bug fixes.

â¸»

ğŸŒŸ Why this project?

Itâ€™s a simple playground for AI agent design.
Instead of jumping straight into big frameworks, you can learn how an agent:
	â€¢	Breaks down a request into steps
	â€¢	Selects and calls tools
	â€¢	Handles errors
	â€¢	Checks its own work

Great for teaching, learning, or hacking on your own AI ideas!

---
